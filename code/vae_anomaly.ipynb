{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IpaU6Gg4NvS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install uproot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twqu3M4K5HEn",
        "outputId": "5a6cd9e3-e3dc-4a55-ef19-9c9f22e7780b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /content/anomaly-detection-in-HEP\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define your workspace directory\n",
        "workspace_dir = '/content/anomaly-detection-in-HEP/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(workspace_dir):\n",
        "    os.makedirs(workspace_dir)\n",
        "\n",
        "# Change the current working directory to your workspace directory\n",
        "os.chdir(workspace_dir)\n",
        "\n",
        "# Verify the current working directory\n",
        "print('Current working directory:', os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "ZyFrN6FwSwpZ",
        "outputId": "2bfdc2e6-0d72-462f-84a3-f36d1bcc45cc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'preprocessing'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6d4e9870ea1b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from preprocessing.dataset import dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from training.trainer import trainer\n",
        "from models.vae import VAE\n",
        "from models.ode import ode\n",
        "from evaluation.evaluator import evaluator\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from copy import copy\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YSTz1IYTeGm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MyJV60oV2ig"
      },
      "source": [
        "## Background data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP64PiDMVUVY"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "from io import BytesIO\n",
        "import uproot\n",
        "\n",
        "def get_data_from_root(file_dir:str, bucket_name:str = 'cuda-programming-406720'):\n",
        "        client = storage.Client()\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(file_dir)\n",
        "        file_contents = BytesIO(blob.download_as_string())\n",
        "        tree = uproot.open(file_contents)\n",
        "        data = tree['Delphes']\n",
        "        return data\n",
        "\n",
        "data_file = 'QCD_LLP_samples/root-files/qcd_100k.root'\n",
        "data = get_data_from_root(data_file)\n",
        "features = ['Track.PT', 'Track.Eta', 'Track.Phi', 'Track.D0', 'Track.DZ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpfrDzH2Vg5p"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data_train = []\n",
        "data_test = []\n",
        "for i in range(5):\n",
        "  d = data['Track'][features[i]].array().tolist()\n",
        "  d = list(chain.from_iterable(d))\n",
        "  data_train.append(d[:20_000])\n",
        "  data_test.append(d[20_000:21_000])\n",
        "data_bkg_train = torch.tensor(data_train).T\n",
        "data_bkg_test = torch.tensor(data_test).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vS44wutkpLF",
        "outputId": "5a7d6c7b-3741-4668-bc1a-7247bbacf92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: torch.Size([20000, 5])\n",
            "Test shape: torch.Size([1000, 5])\n"
          ]
        }
      ],
      "source": [
        "means = data_bkg_train.mean(dim=0)\n",
        "stds = data_bkg_train.std(dim=0)\n",
        "\n",
        "data_bkg_train = (data_bkg_train - means) / stds\n",
        "data_bkg_test =(data_bkg_test - means) / stds\n",
        "print(\"Train shape:\", data_bkg_train.shape)\n",
        "print(\"Test shape:\", data_bkg_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-DEYKs5Tg2K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azoegak2V513"
      },
      "source": [
        "## Signal data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcEmF7waV7T4"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "def get_data_from_h5(file_dir:str, bucket_name:str = 'cuda-programming-406720'):\n",
        "        client = storage.Client()\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(file_dir)\n",
        "        file_contents = BytesIO(blob.download_as_string())\n",
        "        with h5py.File(file_contents, 'r') as f:\n",
        "            dataset = f['Track']\n",
        "            data = dataset[:10_000]\n",
        "            d1 = dataset[:10_000]\n",
        "            d2 = dataset[10_000:10_500]\n",
        "        return d1, d2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbOyxMuZZE_0"
      },
      "outputs": [],
      "source": [
        "sig1_train, sig1_test = get_data_from_h5('QCD_LLP_samples/h5-files/500GeV_n3_events_100k_1mm_pileup.h5')\n",
        "sig2_train, sig2_test = get_data_from_h5('QCD_LLP_samples/h5-files/100GeV_n3_events_100k_1mm_pileup.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjrdapG4ZZx4"
      },
      "outputs": [],
      "source": [
        "data_sig_train = np.concatenate((sig1_train,sig2_train))\n",
        "data_sig_test = np.concatenate((sig1_test,sig2_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laaZB3YUaXoq"
      },
      "outputs": [],
      "source": [
        "data_sig_train = torch.tensor([list(data_sig_train[i]) for i in range(data_sig_train.shape[0])])\n",
        "data_sig_test = torch.tensor([list(data_sig_test[i]) for i in range(data_sig_test.shape[0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g94g-bb1kSaE",
        "outputId": "2979fada-1a78-4cf8-e185-dcd48b843d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: torch.Size([20000, 5])\n",
            "Test shape: torch.Size([1000, 5])\n"
          ]
        }
      ],
      "source": [
        "data_sig_train = (data_sig_train - means) / stds\n",
        "data_sig_test =(data_sig_test - means) / stds\n",
        "\n",
        "print(\"Train shape:\", data_sig_train.shape)\n",
        "print(\"Test shape:\", data_sig_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRq4xUM8UILv"
      },
      "source": [
        "# Train VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4K9qrjRTcz1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFTRAM7YN5al",
        "outputId": "39f8eb3c-e0bc-4352-a398-1cfd0475aab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-05-15 22:05:42.509938: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-15 22:05:42.509992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-15 22:05:42.511495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-15 22:05:42.519502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-15 22:05:43.552195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using h5 files\n",
            "Signal file: QCD_LLP_samples/h5-files/500GeV_n3_events_100k_1mm_pileup.h5\n",
            "Signal file: QCD_LLP_samples/h5-files/100GeV_n3_events_100k_1mm_pileup.h5\n",
            "Background file: QCD_LLP_samples/h5-files/QCD_multijet_events_200k_pileup.h5\n",
            "Using root files: QCD_LLP_samples/root-files/qcd_100k.root\n",
            "(8305, 5)\n",
            "(6644, 5)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 512]           3,072\n",
            "              ReLU-2                  [-1, 512]               0\n",
            "           Dropout-3                  [-1, 512]               0\n",
            "            Linear-4                    [-1, 2]           1,026\n",
            "           Dropout-5                    [-1, 2]               0\n",
            "            Linear-6                    [-1, 2]           1,026\n",
            "           Dropout-7                    [-1, 2]               0\n",
            "            Linear-8                  [-1, 512]           1,536\n",
            "              ReLU-9                  [-1, 512]               0\n",
            "           Linear-10                    [-1, 5]           2,565\n",
            "          Dropout-11                    [-1, 5]               0\n",
            "             Tanh-12                    [-1, 5]               0\n",
            "================================================================\n",
            "Total params: 9,225\n",
            "Trainable params: 9,225\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.05\n",
            "----------------------------------------------------------------\n",
            "Device:  cuda\n",
            "Epoch: 0, Batch Size: 2, Train loss: 0.2520, Valid loss: 0.2469, Time elapsed (hh:mm:ss.ms) 0:00:03.580167\n",
            "Epoch: 1, Batch Size: 2, Train loss: 0.2356, Valid loss: 0.2154, Time elapsed (hh:mm:ss.ms) 0:00:03.209837\n",
            "Epoch: 2, Batch Size: 2, Train loss: 0.2215, Valid loss: 0.2155, Time elapsed (hh:mm:ss.ms) 0:00:03.253293\n",
            "Epoch: 3, Batch Size: 2, Train loss: 0.2163, Valid loss: 0.2137, Time elapsed (hh:mm:ss.ms) 0:00:03.349191\n",
            "Epoch: 4, Batch Size: 2, Train loss: 0.2128, Valid loss: 0.2151, Time elapsed (hh:mm:ss.ms) 0:00:03.308088\n",
            "Epoch: 5, Batch Size: 2, Train loss: 0.2122, Valid loss: 0.2119, Time elapsed (hh:mm:ss.ms) 0:00:03.239078\n",
            "Epoch: 6, Batch Size: 2, Train loss: 0.2116, Valid loss: 0.2111, Time elapsed (hh:mm:ss.ms) 0:00:03.225786\n",
            "Epoch: 7, Batch Size: 2, Train loss: 0.2103, Valid loss: 0.2116, Time elapsed (hh:mm:ss.ms) 0:00:03.202846\n",
            "Epoch: 8, Batch Size: 2, Train loss: 0.2107, Valid loss: 0.2091, Time elapsed (hh:mm:ss.ms) 0:00:03.213534\n",
            "Epoch: 9, Batch Size: 2, Train loss: 0.2080, Valid loss: 0.1986, Time elapsed (hh:mm:ss.ms) 0:00:03.244357\n",
            "Epoch: 10, Batch Size: 2, Train loss: 0.1890, Valid loss: 0.2312, Time elapsed (hh:mm:ss.ms) 0:00:03.193876\n",
            "Epoch: 11, Batch Size: 2, Train loss: 0.2081, Valid loss: 0.1962, Time elapsed (hh:mm:ss.ms) 0:00:03.195847\n",
            "Epoch: 12, Batch Size: 2, Train loss: 0.1972, Valid loss: 0.2010, Time elapsed (hh:mm:ss.ms) 0:00:03.208246\n",
            "Epoch: 13, Batch Size: 2, Train loss: 0.1996, Valid loss: 0.1954, Time elapsed (hh:mm:ss.ms) 0:00:03.235932\n",
            "Epoch: 14, Batch Size: 2, Train loss: 0.1916, Valid loss: 0.1592, Time elapsed (hh:mm:ss.ms) 0:00:03.193805\n",
            "Epoch: 15, Batch Size: 2, Train loss: 0.1399, Valid loss: 0.2012, Time elapsed (hh:mm:ss.ms) 0:00:03.191827\n",
            "Epoch: 16, Batch Size: 2, Train loss: 0.1557, Valid loss: 0.1392, Time elapsed (hh:mm:ss.ms) 0:00:03.219303\n",
            "Epoch: 17, Batch Size: 2, Train loss: 0.1340, Valid loss: 0.1090, Time elapsed (hh:mm:ss.ms) 0:00:03.219651\n",
            "Epoch: 18, Batch Size: 2, Train loss: 0.1033, Valid loss: 0.0756, Time elapsed (hh:mm:ss.ms) 0:00:03.223443\n",
            "Epoch: 19, Batch Size: 2, Train loss: 0.0825, Valid loss: 0.0809, Time elapsed (hh:mm:ss.ms) 0:00:03.289553\n",
            "Epoch: 20, Batch Size: 2, Train loss: 0.0777, Valid loss: 0.0780, Time elapsed (hh:mm:ss.ms) 0:00:03.242975\n",
            "Epoch: 21, Batch Size: 2, Train loss: 0.0812, Valid loss: 0.0767, Time elapsed (hh:mm:ss.ms) 0:00:03.232299\n",
            "Epoch: 22, Batch Size: 2, Train loss: 0.0717, Valid loss: 0.0622, Time elapsed (hh:mm:ss.ms) 0:00:03.265083\n",
            "Epoch: 23, Batch Size: 2, Train loss: 0.0649, Valid loss: 0.0654, Time elapsed (hh:mm:ss.ms) 0:00:03.208632\n",
            "Epoch: 24, Batch Size: 2, Train loss: 0.0652, Valid loss: 0.0601, Time elapsed (hh:mm:ss.ms) 0:00:03.209781\n",
            "Epoch: 25, Batch Size: 2, Train loss: 0.0602, Valid loss: 0.0606, Time elapsed (hh:mm:ss.ms) 0:00:03.218869\n",
            "Epoch: 26, Batch Size: 2, Train loss: 0.0610, Valid loss: 0.0580, Time elapsed (hh:mm:ss.ms) 0:00:03.195958\n",
            "Epoch: 27, Batch Size: 2, Train loss: 0.0586, Valid loss: 0.0542, Time elapsed (hh:mm:ss.ms) 0:00:03.211483\n",
            "Epoch: 28, Batch Size: 2, Train loss: 0.0552, Valid loss: 0.0556, Time elapsed (hh:mm:ss.ms) 0:00:03.226266\n",
            "Epoch: 29, Batch Size: 2, Train loss: 0.0571, Valid loss: 0.0518, Time elapsed (hh:mm:ss.ms) 0:00:03.210883\n",
            "Epoch: 30, Batch Size: 2, Train loss: 0.0532, Valid loss: 0.0523, Time elapsed (hh:mm:ss.ms) 0:00:03.246991\n",
            "Epoch: 31, Batch Size: 2, Train loss: 0.0518, Valid loss: 0.0519, Time elapsed (hh:mm:ss.ms) 0:00:03.244592\n",
            "Epoch: 32, Batch Size: 2, Train loss: 0.0513, Valid loss: 0.0472, Time elapsed (hh:mm:ss.ms) 0:00:03.208319\n",
            "Epoch: 33, Batch Size: 2, Train loss: 0.0478, Valid loss: 0.0452, Time elapsed (hh:mm:ss.ms) 0:00:03.263322\n",
            "Epoch: 34, Batch Size: 2, Train loss: 0.0455, Valid loss: 0.0425, Time elapsed (hh:mm:ss.ms) 0:00:03.194757\n",
            "Epoch: 35, Batch Size: 2, Train loss: 0.0434, Valid loss: 0.0410, Time elapsed (hh:mm:ss.ms) 0:00:03.267030\n",
            "Epoch: 36, Batch Size: 2, Train loss: 0.0414, Valid loss: 0.0391, Time elapsed (hh:mm:ss.ms) 0:00:03.227415\n",
            "Epoch: 37, Batch Size: 2, Train loss: 0.0396, Valid loss: 0.0398, Time elapsed (hh:mm:ss.ms) 0:00:03.246929\n",
            "Epoch: 38, Batch Size: 2, Train loss: 0.0395, Valid loss: 0.0389, Time elapsed (hh:mm:ss.ms) 0:00:03.260795\n",
            "Epoch: 39, Batch Size: 2, Train loss: 0.0395, Valid loss: 0.0386, Time elapsed (hh:mm:ss.ms) 0:00:03.246050\n",
            "Epoch: 40, Batch Size: 2, Train loss: 0.0392, Valid loss: 0.0373, Time elapsed (hh:mm:ss.ms) 0:00:03.247773\n",
            "Epoch: 41, Batch Size: 2, Train loss: 0.0383, Valid loss: 0.0370, Time elapsed (hh:mm:ss.ms) 0:00:03.246640\n",
            "Epoch: 42, Batch Size: 2, Train loss: 0.0369, Valid loss: 0.0339, Time elapsed (hh:mm:ss.ms) 0:00:03.245916\n",
            "Epoch: 43, Batch Size: 2, Train loss: 0.0340, Valid loss: 0.0321, Time elapsed (hh:mm:ss.ms) 0:00:03.246598\n",
            "Epoch: 44, Batch Size: 2, Train loss: 0.0316, Valid loss: 0.0285, Time elapsed (hh:mm:ss.ms) 0:00:03.236011\n",
            "Epoch: 45, Batch Size: 2, Train loss: 0.0279, Valid loss: 0.0216, Time elapsed (hh:mm:ss.ms) 0:00:03.255945\n",
            "Epoch: 46, Batch Size: 2, Train loss: 0.0186, Valid loss: 0.0106, Time elapsed (hh:mm:ss.ms) 0:00:03.232666\n",
            "Epoch: 47, Batch Size: 2, Train loss: 0.0130, Valid loss: 0.0215, Time elapsed (hh:mm:ss.ms) 0:00:03.257764\n",
            "Epoch: 48, Batch Size: 2, Train loss: 0.0193, Valid loss: 0.0135, Time elapsed (hh:mm:ss.ms) 0:00:03.219324\n",
            "Epoch: 49, Batch Size: 2, Train loss: 0.0126, Valid loss: 0.0109, Time elapsed (hh:mm:ss.ms) 0:00:03.236907\n",
            "Epoch: 50, Batch Size: 2, Train loss: 0.0129, Valid loss: 0.0145, Time elapsed (hh:mm:ss.ms) 0:00:03.243640\n",
            "Epoch: 51, Batch Size: 2, Train loss: 0.0135, Valid loss: 0.0112, Time elapsed (hh:mm:ss.ms) 0:00:03.206767\n",
            "Epoch: 52, Batch Size: 2, Train loss: 0.0113, Valid loss: 0.0079, Time elapsed (hh:mm:ss.ms) 0:00:03.207862\n",
            "Epoch: 53, Batch Size: 2, Train loss: 0.0093, Valid loss: 0.0119, Time elapsed (hh:mm:ss.ms) 0:00:03.230352\n",
            "Epoch: 54, Batch Size: 2, Train loss: 0.0103, Valid loss: 0.0082, Time elapsed (hh:mm:ss.ms) 0:00:03.236618\n",
            "Epoch: 55, Batch Size: 2, Train loss: 0.0089, Valid loss: 0.0087, Time elapsed (hh:mm:ss.ms) 0:00:03.216156\n",
            "Epoch: 56, Batch Size: 2, Train loss: 0.0090, Valid loss: 0.0088, Time elapsed (hh:mm:ss.ms) 0:00:03.228602\n",
            "Epoch: 57, Batch Size: 2, Train loss: 0.0091, Valid loss: 0.0071, Time elapsed (hh:mm:ss.ms) 0:00:03.283222\n",
            "Epoch: 58, Batch Size: 2, Train loss: 0.0079, Valid loss: 0.0075, Time elapsed (hh:mm:ss.ms) 0:00:03.230388\n",
            "Epoch: 59, Batch Size: 2, Train loss: 0.0083, Valid loss: 0.0083, Time elapsed (hh:mm:ss.ms) 0:00:03.256164\n",
            "Epoch: 60, Batch Size: 2, Train loss: 0.0077, Valid loss: 0.0068, Time elapsed (hh:mm:ss.ms) 0:00:03.251973\n",
            "Epoch: 61, Batch Size: 2, Train loss: 0.0077, Valid loss: 0.0086, Time elapsed (hh:mm:ss.ms) 0:00:03.238720\n",
            "Epoch: 62, Batch Size: 2, Train loss: 0.0084, Valid loss: 0.0083, Time elapsed (hh:mm:ss.ms) 0:00:03.219677\n",
            "Epoch: 63, Batch Size: 2, Train loss: 0.0079, Valid loss: 0.0073, Time elapsed (hh:mm:ss.ms) 0:00:03.246589\n",
            "Epoch: 64, Batch Size: 2, Train loss: 0.0071, Valid loss: 0.0069, Time elapsed (hh:mm:ss.ms) 0:00:03.223430\n",
            "Epoch: 65, Batch Size: 2, Train loss: 0.0080, Valid loss: 0.0067, Time elapsed (hh:mm:ss.ms) 0:00:03.223481\n",
            "Epoch: 66, Batch Size: 2, Train loss: 0.0074, Valid loss: 0.0063, Time elapsed (hh:mm:ss.ms) 0:00:03.239236\n",
            "Epoch: 67, Batch Size: 2, Train loss: 0.0070, Valid loss: 0.0070, Time elapsed (hh:mm:ss.ms) 0:00:03.213789\n",
            "Epoch: 68, Batch Size: 2, Train loss: 0.0071, Valid loss: 0.0058, Time elapsed (hh:mm:ss.ms) 0:00:03.224455\n",
            "Epoch: 69, Batch Size: 2, Train loss: 0.0070, Valid loss: 0.0063, Time elapsed (hh:mm:ss.ms) 0:00:03.221780\n",
            "Figure(640x480)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/anomaly-detection-in-HEP/code/main.py\", line 90, in <module>\n",
            "    main()\n",
            "  File \"/content/anomaly-detection-in-HEP/code/main.py\", line 64, in main\n",
            "    evaluator_vae = evaluator(model=model_vae, test_data=data.data_test, labels=data.labels_test)\n",
            "AttributeError: 'dataset' object has no attribute 'labels_test'\n"
          ]
        }
      ],
      "source": [
        "!python code/main.py vae"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
