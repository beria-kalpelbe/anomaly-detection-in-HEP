{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Bucket: cuda-programming-406720>\n",
      "b'x,y\\n5.4881350392732475,29.65450786127179\\n7.151893663724195,34.99358978951999\\n6.027633760716439,35.99427342345802\\n5.448831829968968,24.815775427356154\\n4.236547993389047,21.034959534504537\\n6.458941130666561,29.44885970151621\\n4.375872112626925,26.03457170516168\\n8.917730007820797,40.20383605668101\\n9.636627605010293,45.209618148604235\\n3.8344151882577773,19.781916673047135\\n7.917250380826646,35.25869735976178\\n5.288949197529044,24.25954820175923\\n5.680445610939323,31.285078338410024\\n9.25596638292661,40.40569504519258\\n0.7103605819788694,6.807265944716013\\n0.8712929970154071,9.17242369467127\\n0.2021839744032572,11.018668196092381\\n8.32619845547938,36.27580657984814\\n7.781567509498505,41.50571911298206\\n8.700121482468191,37.761470008812395\\n9.78618342232764,50.90932833944452\\n7.9915856421672355,38.65285458514596\\n4.6147936225293185,30.226785372020643\\n7.805291762864554,39.269622248908675\\n1.1827442586893322,14.123612828740345\\n6.399210213275238,27.98871877564416\\n1.433532874090464,10.562201122125952\\n9.446689170495839,40.98872229411504\\n5.218483217500717,25.835334845224317\\n4.146619399905235,19.773754789163384\\n2.64555612104627,15.76205627812484\\n7.742336894342166,37.11197752251537\\n4.5615033221654855,20.887488252149787\\n5.684339488686485,31.66207914844614\\n0.18789800436355142,8.417606559519957\\n6.176354970758771,29.359314792429537\\n6.120957227224214,31.716309443563855\\n6.169339968747569,27.616764982574693\\n9.437480785146242,45.509388096146765\\n6.818202991034834,38.565773939901476\\n3.59507900573786,19.566005547464677\\n4.3703195379934145,26.155381951610476\\n6.976311959272649,31.223226461134516\\n0.6022547162926983,11.572290906356448\\n6.667667154456677,31.564729547298718\\n6.706378696181594,30.657428404797546\\n2.103825610738409,16.28043179105447\\n1.289262976548533,7.358127368069068\\n3.1542835092418384,22.906534329140985\\n3.637107709426226,16.595385599630376\\n'\n"
     ]
    }
   ],
   "source": [
    "import google.auth\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "credentials, project = google.auth.default()  \n",
    "#List buckets using the default account on the current gcloud cli\n",
    "client = storage.Client(credentials=credentials)\n",
    "buckets = client.list_buckets()\n",
    "for bucket in buckets:\n",
    "    print(bucket)\n",
    "\n",
    "#custom function to read data from cvs/sql/excel/xml file\n",
    "def read_file_gcs(bucket_name, general_file_name):\n",
    "    \"\"\"\n",
    "    Read a file from the bucket\n",
    "    \"\"\"\n",
    "\n",
    "    # create storage client\n",
    "    client = storage.Client()\n",
    "    # get bucket with name\n",
    "    BUCKET= client.get_bucket(bucket_name)\n",
    "    # get bucket data as blob\n",
    "    blob = BUCKET.blob(general_file_name)\n",
    "    assert blob.exists()  # Will be false if the next line would raise NotFound\n",
    "    # convert to string\n",
    "    data = blob.download_as_string()\n",
    "    return data\n",
    "\n",
    "bucket_name = \"cuda-programming-406720\"\n",
    "general_file_name = \"tutorial_1.csv\"  #cloudsql/table_creation.sql  sample.yml\n",
    "data = read_file_gcs(bucket_name, general_file_name)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "# Replace 'your_bucket_name' and 'your_file_name.h5' with your actual bucket and file names\n",
    "bucket_name = 'cuda-programming-406720'\n",
    "file_name = 'QCD_LLP_samples/h5-files/100GeV_n3_events_100k_1mm_pileup.h5'\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Get the bucket\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Get the blob (file) from the bucket\n",
    "blob = bucket.blob(file_name)\n",
    "\n",
    "# Read the file directly into memory\n",
    "file_contents = BytesIO(blob.download_as_string())\n",
    "\n",
    "# Open the .h5 file using h5py\n",
    "with h5py.File(file_contents, 'r') as f:\n",
    "    # Now you can read datasets, attributes, etc. from the file\n",
    "    print(f.keys())\n",
    "    dataset = f['Track']  # Replace 'dataset_name' with the actual dataset name\n",
    "    lenght_data = f['lengths'][:1000]\n",
    "    data = dataset[:1000]  # Read the entire dataset into memory (you can also read specific parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.08710044e-04, -8.56699944e+00, -3.13911176e+00, -6.72592163e+01,\n",
       "       -1.92415161e+02])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([list(element) for element in data.tolist()])\n",
    "np.min(d, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "\n",
    "\n",
    "append_fields(data, 'label', np.zeros((data.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0Copying from <STDIN>...\n",
      "100  378M  100  378M    0     0   427k      0  0:15:06  0:15:06 --:--:--  922k  0  0:26:45  0:01:39  0:25:06  161k206k283k      0  0:22:49  0:04:35  0:18:14  276k/ [0 files][264.0 KiB/    0.0 B]                                                / [0 files][100.0 MiB/    0.0 B]      0.0 B/s                                   / [0 files][200.1 MiB/    0.0 B]      0.0 B/s                                   \n",
      "/ [1 files][    0.0 B/    0.0 B]      0.0 B/s                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!curl https://cernbox.cern.ch/remote.php/dav/public-files/ry8Zv08AGIH4fWt/h5_files/QCD_multijet_events_200k.h5 | gsutil cp - gs://cuda-programming-406720/QCD_multijet_events_200k.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0Copying from <STDIN>...\n",
      "100 1124M  100 1124M    0     0   747k      0  0:25:39  0:25:39 --:--:-- 1217k  0:17 --:--:--     0/ [0 files][264.0 KiB/    0.0 B]                                                / [0 files][100.0 MiB/    0.0 B]      0.0 B/s                                   684k      0  0:28:02  0:05:15  0:22:47  800k/ [0 files][200.1 MiB/    0.0 B]      0.0 B/s                                   / [0 files][300.1 MiB/    0.0 B]      0.0 B/s                                   / [0 files][400.1 MiB/    0.0 B]      0.0 B/s                                   / [0 files][500.2 MiB/    0.0 B]      0.0 B/s                                   / [0 files][600.2 MiB/    0.0 B]      0.0 B/s                                   / [0 files][700.2 MiB/    0.0 B]      0.0 B/s                                   / [0 files][800.2 MiB/    0.0 B]      0.0 B/s                                   04:21  953k4:25  0:20:26  0:03:59  900k/ [0 files][900.0 MiB/    0.0 B]      0.0 B/s                                     925k22:41  0:02:27  903k6 1043k/ [0 files][ 1000 MiB/    0.0 B]      0.0 B/s                                   \n",
      "/ [1 files][    0.0 B/    0.0 B]      0.0 B/s                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!curl https://cernbox.cern.ch/remote.php/dav/public-files/ry8Zv08AGIH4fWt/h5_files/QCD_multijet_events_200k_pileup.h5 | gsutil cp - gs://cuda-programming-406720/QCD_LLP_samples/h5-files/QCD_multijet_events_200k_pileup.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Copying from <STDIN>...\n",
      " 42 1124M   42  475M    0     0   269k      0  1:11:18  0:30:07  0:41:11  433k   0      0 --:--:--  0:04:28 --:--:--     0:--:--  0:07:31 --:--:--     0--  0:10:00 --:--:--     0--:--  0:10:50 --:--:--     0     0-  0:16:06 --:--:--     0 0     0    0     0      0      0 --:--:--  0:17:39 --:--:--     03 1073k/ [0 files][264.0 KiB/    0.0 B]                                                  599k/ [0 files][100.0 MiB/    0.0 B]      0.0 B/s                                   / [0 files][200.1 MiB/    0.0 B]      0.0 B/s                                   04  732k 0:27:06  0:51:34 2280k/ [0 files][300.1 MiB/    0.0 B]      0.0 B/s                                   \n",
      "curl: (18) transfer closed with 680614096 bytes remaining to read\n",
      "/ [1 files][    0.0 B/    0.0 B]      0.0 B/s                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Copying from <STDIN>...\n",
      "100  574M  100  574M    0     0   811k      0  0:12:04  0:12:04 --:--:-- 2242k  / [0 files][264.0 KiB/    0.0 B]                                                / [0 files][100.0 MiB/    0.0 B]      0.0 B/s                                     0:08:47 1394k/ [0 files][200.1 MiB/    0.0 B]      0.0 B/s                                   / [0 files][300.1 MiB/    0.0 B]      0.0 B/s                                     726k      0  0:13:30  0:09:33  0:03:57  605k/ [0 files][400.1 MiB/    0.0 B]      0.0 B/s                                   0:51 2815k\n",
      "/ [1 files][    0.0 B/    0.0 B]      0.0 B/s                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0Copying from <STDIN>...\n",
      "100  574M  100  574M    0     0   915k      0  0:10:43  0:10:43 --:--:-- 1089k  1628k/ [0 files][264.0 KiB/    0.0 B]                                                / [0 files][100.0 MiB/    0.0 B]      0.0 B/s                                   / [0 files][200.1 MiB/    0.0 B]      0.0 B/s                                   / [0 files][300.1 MiB/    0.0 B]      0.0 B/s                                   0:48  0:07:34  0:03:14  125k/ [0 files][400.1 MiB/    0.0 B]      0.0 B/s                                   \n",
      "/ [1 files][    0.0 B/    0.0 B]      0.0 B/s                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!curl https://cernbox.cern.ch/remote.php/dav/public-files/ry8Zv08AGIH4fWt/h5_files/500GeV_n3_events_100k_1mm_pileup.h5 | gsutil cp - gs://cuda-programming-406720/QCD_LLP_samples/h5-files/500GeV_n3_events_100k_1mm_pileup.h5\n",
    "!curl https://cernbox.cern.ch/remote.php/dav/public-files/ry8Zv08AGIH4fWt/h5_files/100GeV_n3_events_100k_1mm_pileup.h5 | gsutil cp - gs://cuda-programming-406720/QCD_LLP_samples/h5-files/100GeV_n3_events_100k_1mm_pileup.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beria/Documents/anomaly-detection/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.utils import BoxUniform\n",
    "from torch.distributions import Normal\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load()\n",
    "\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6663, -0.0535,  0.0797, -0.1945,  0.0567],\n",
       "        [-0.7157, -0.1152, -0.9369, -0.1945,  0.0567],\n",
       "        [-0.7858, -0.1158, -0.9405, -0.1945,  0.0567],\n",
       "        ...,\n",
       "        [-0.9936, -0.2782, -0.9517, -0.1945, -0.2557],\n",
       "        [-0.9937, -0.4728,  0.7841, -0.1945, -0.1977],\n",
       "        [-0.9938,  0.2047,  0.5357, -0.1945, -0.0633]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing.dataset import dataset\n",
    "\n",
    "mydata = dataset(data_file='/home/beria/Documents/anomaly-detection/data-clf.csv')\n",
    "mydata.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.data = (mydata.data - mydata.data.mean(dim=0)) / mydata.data.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2033e+01, -1.1467e-01,  1.3517e-01,  6.4417e-03,  1.4279e+00],\n",
       "        [ 1.0201e+01, -3.0398e-01, -1.6240e+00,  6.4417e-03,  1.4279e+00],\n",
       "        [ 7.6021e+00, -3.0566e-01, -1.6302e+00,  6.4417e-03,  1.4279e+00],\n",
       "        ...,\n",
       "        [-1.0682e-01, -8.0410e-01, -1.6496e+00,  6.4417e-03, -1.2058e+00],\n",
       "        [-1.0930e-01, -1.4009e+00,  1.3541e+00,  6.4417e-03, -7.1645e-01],\n",
       "        [-1.1162e-01,  6.7737e-01,  9.2425e-01,  6.4417e-03,  4.1600e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultivariateGaussianMDN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multivariate Gaussian MDN with diagonal Covariance matrix.\n",
    "\n",
    "    For a documented version of this code, see:\n",
    "    https://github.com/mackelab/pyknos/blob/main/pyknos/mdn/mdn.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        hidden_net,\n",
    "        num_components,\n",
    "        hidden_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._features = features\n",
    "        self._num_components = num_components\n",
    "\n",
    "        self._hidden_net = hidden_net\n",
    "        self._logits_layer = nn.Linear(hidden_features, num_components)\n",
    "        self._means_layer = nn.Linear(hidden_features, num_components * features)\n",
    "        self._unconstrained_diagonal_layer = nn.Linear(\n",
    "            hidden_features, num_components * features\n",
    "        )\n",
    "\n",
    "    def get_mixture_components(self, context):\n",
    "        h = self._hidden_net(context)\n",
    "\n",
    "        # mixture coefficients in log space\n",
    "        logits = self._logits_layer(h)\n",
    "        logits = logits - torch.logsumexp(logits, dim=1).unsqueeze(1)  # normalization\n",
    "\n",
    "        # means\n",
    "        means = self._means_layer(h).view(-1, self._num_components, self._features)\n",
    "\n",
    "        # log variances for diagonal Cov matrix\n",
    "        # otherwise: Cholesky decomposition s.t. Cov = AA^T, A is lower triangular.\n",
    "        log_variances = self._unconstrained_diagonal_layer(h).view(\n",
    "            -1, self._num_components, self._features\n",
    "        )\n",
    "        variances = torch.exp(log_variances)\n",
    "\n",
    "        return logits, means, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mog_log_prob(theta, logits, means, variances):\n",
    "    \"\"\" Log probability of a mixture of Gaussians.\n",
    "        args:\n",
    "            theta: parameters\n",
    "            logits: log mixture coefficients\n",
    "            means: means of the Gaussians\n",
    "            variances: variances of the Gaussians\n",
    "        returns:\n",
    "            log probability of the mixture of Gaussians\"\"\"\n",
    "    _, _, theta_dim = means.size()\n",
    "    theta = theta.view(-1, 1, theta_dim)\n",
    "\n",
    "    log_cov_det = -0.5 * torch.log(torch.prod(variances, dim=2))\n",
    "\n",
    "    a = logits\n",
    "    b = -(theta_dim / 2.0) * math.log(2 * math.pi)\n",
    "    c = log_cov_det\n",
    "    d1 = theta.expand_as(means) - means\n",
    "    precisions = 1.0 / variances\n",
    "    exponent = torch.sum(d1 * precisions * d1, dim=2)\n",
    "    exponent = torch.tensor(-0.5) * exponent\n",
    "\n",
    "    return torch.logsumexp(a + b + c + exponent, dim=-1)\n",
    "\n",
    "\n",
    "def mog_sample(logits, means, variances):\n",
    "    \"\"\"Sample from a mixture of Gaussians.\n",
    "        args:\n",
    "            logits: log mixture coefficients\n",
    "            means: means of the Gaussians\n",
    "            variances: variances of the Gaussians   \n",
    "        returns:\n",
    "            samples from the mixture of Gaussians\"\"\"\n",
    "    \n",
    "    coefficients = F.softmax(logits, dim=-1)\n",
    "    choices = torch.multinomial(coefficients, num_samples=1, replacement=True).view(-1)\n",
    "    chosen_means = means[0, choices, :]  # 0 for first batch position\n",
    "    chosen_variances = variances[0, choices, :]\n",
    "\n",
    "    _, _, output_dim = means.shape\n",
    "    standard_normal_samples = torch.randn(output_dim)\n",
    "    zero_mean_samples = standard_normal_samples * torch.sqrt(chosen_variances)\n",
    "    samples = chosen_means + zero_mean_samples\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = mydata.data[mydata.labels==1]\n",
    "background_data = mydata.data[mydata.labels==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal distribution learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(signal_data, signal_data)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=50,\n",
    ")\n",
    "\n",
    "\n",
    "###########\n",
    "hidden_net = nn.Sequential(\n",
    "    nn.Linear(5, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "###########\n",
    "\n",
    "mdn_signal = MultivariateGaussianMDN(\n",
    "    features=5,\n",
    "    hidden_net=hidden_net,\n",
    "    num_components=5,\n",
    "    hidden_features=30,\n",
    ")\n",
    "opt = optim.Adam(mdn_signal.parameters(), lr=0.000001)\n",
    "for e in range(5):\n",
    "    for x_batch, theta_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        logits, means, variances = mdn_signal.get_mixture_components(x_batch)\n",
    "        log_probs = mog_log_prob(theta_batch, logits, means, variances)\n",
    "        loss = -log_probs.sum()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-57.1628, -46.2835, -30.2737,  ...,  -6.1329,  -5.7397,  -6.2218],\n",
       "       grad_fn=<LogsumexpBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, means, variances = mdn_signal.get_mixture_components(mydata.data[mydata.labels==1])\n",
    "p = mog_log_prob(mydata.data[mydata.labels==1], logits, means, variances)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background distribution learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(background_data, background_data)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=50,\n",
    ")\n",
    "\n",
    "\n",
    "###########\n",
    "hidden_net = nn.Sequential(\n",
    "    nn.Linear(5, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "###########\n",
    "\n",
    "mdn_background = MultivariateGaussianMDN(\n",
    "    features=5,\n",
    "    hidden_net=hidden_net,\n",
    "    num_components=5,\n",
    "    hidden_features=30,\n",
    ")\n",
    "opt = optim.Adam(mdn_background.parameters(), lr=0.000001)\n",
    "for e in range(5):\n",
    "    for x_batch, theta_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        logits, means, variances = mdn_background.get_mixture_components(x_batch)\n",
    "        log_probs = mog_log_prob(theta_batch, logits, means, variances)\n",
    "        loss = -log_probs.sum()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over-density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "custom_loss() missing 1 required positional argument: 'x_recon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m     54\u001b[0m y_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mydata\u001b[38;5;241m.\u001b[39mlabels)\u001b[38;5;66;03m#.view(-1,1)\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmydata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mTypeError\u001b[0m: custom_loss() missing 1 required positional argument: 'x_recon'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the likelihood ratio (ground truth)\n",
    "def p_sig(x):\n",
    "    logits, means, variances = mdn_signal.get_mixture_components(x)\n",
    "    p = mog_log_prob(x, logits, means, variances)\n",
    "    return p\n",
    "\n",
    "def p_bg(x):\n",
    "    logits, means, variances = mdn_background.get_mixture_components(x)\n",
    "    p = mog_log_prob(x, logits, means, variances)\n",
    "    return p\n",
    "\n",
    "def likelihood_ratio(X):\n",
    "    return p_sig(X) / p_bg(X)\n",
    "\n",
    "def ams(x):\n",
    "    return torch.mean(2*((p_sig(x) + p_bg(x))*torch.log(1 + p_sig(x)/p_bg(x)) - p_sig(x)))\n",
    "\n",
    "\n",
    "\n",
    "# Define the classifier model\n",
    "class LikelihoodRatioClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LikelihoodRatioClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Initialize the classifier\n",
    "input_size = mydata.data.shape[1]  # Assuming X is a 2D tensor\n",
    "classifier = LikelihoodRatioClassifier(input_size)\n",
    "\n",
    "# Define the loss function\n",
    "def custom_loss(y_pred, y_true, x, Lambda: float= 0.4):\n",
    "    # likelihood_ratios = likelihood_ratio(mydata.data)\n",
    "    return nn.BCELoss()(y_pred, y_true) + Lambda*(1/likelihood_ratio(x) - 1 + y_pred)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = classifier(mydata.data)\n",
    "    y_pred = (y_pred > 0.5).int()\n",
    "    y_true = torch.tensor(mydata.labels)#.view(-1,1)\n",
    "    loss = custom_loss(y_pred.view(-1), y_true, mydata.data, )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Use the trained classifier for anomaly detection\n",
    "# classifier.eval()\n",
    "# y_pred = classifier(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2033e+01, -1.1467e-01,  1.3517e-01,  6.4417e-03,  1.4279e+00])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.2678, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ams(mydata.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/anomaly-detection/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/anomaly-detection/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/anomaly-detection/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/anomaly-detection/venv/lib/python3.12/site-packages/torch/nn/functional.py:3154\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3152\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Int"
     ]
    }
   ],
   "source": [
    "nn.BCELoss()(y_pred.view(-1), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
